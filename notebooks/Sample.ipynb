{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d36d537-9449-423c-a470-f4d1717f7e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n"
     ]
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pickle   # –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π sklearn –≤ —Ñ–æ—Ä–º–∞—Ç–µ pickle (–µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è)\n",
    "import joblib   # –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π, —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Å joblib (–µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è)\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (CPU –∏–ª–∏ GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe2ae2c-3d52-4dc9-a296-722b53ddefbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å Transformers –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏.\n",
      "‚ÑπÔ∏è –ü–æ–ª—å–∑—É–µ–º—Å—è sklearn/keras/PyTorch state-dict, –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –±—É–¥–µ—Ç –Ω–∞ CPU.\n",
      "\n",
      "== –ò—Ç–æ–≥–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ==\n",
      "Model object type: <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_DIR = '../models/final_model'\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    raise FileNotFoundError(f\"–ö–∞—Ç–∞–ª–æ–≥ —Å –º–æ–¥–µ–ª—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω: {MODEL_DIR}\")\n",
    "\n",
    "# 1) –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ HF-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä\n",
    "model = None\n",
    "tokenizer = None\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å Transformers –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏.\")\n",
    "except Exception as hf_err:\n",
    "    print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ Transformers-–º–æ–¥–µ–ª—å:\", hf_err)\n",
    "\n",
    "# 2) –µ—Å–ª–∏ –Ω–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä ‚Äî –∏—â–µ–º —Ñ–∞–π–ª—ã —Å –¥—Ä—É–≥–∏–º–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏\n",
    "if model is None:\n",
    "    # —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ\n",
    "    files = os.listdir(MODEL_DIR)\n",
    "    # –∏—â–µ–º PyTorch state-dict (.pt/.pth)\n",
    "    pt_files = [f for f in files if f.endswith(('.pt','.pth'))]\n",
    "    # –∏—â–µ–º sklearn pickle/joblib (.pkl/.joblib/.sav)\n",
    "    pkl_files = [f for f in files if f.endswith(('.pkl','.joblib','.sav'))]\n",
    "    # –∏—â–µ–º Keras H5 (.h5)\n",
    "    h5_files  = [f for f in files if f.endswith('.h5')]\n",
    "    \n",
    "    if pt_files:\n",
    "        pt_path = os.path.join(MODEL_DIR, pt_files[0])\n",
    "        try:\n",
    "            model = torch.load(pt_path, map_location='cpu')\n",
    "            print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω PyTorch-–º–æ–¥–µ–ª—å –∏–∑ —Ñ–∞–π–ª–∞ {pt_files[0]} –Ω–∞ CPU.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ PyTorch-–º–æ–¥–µ–ª–∏ {pt_files[0]}:\", e)\n",
    "    elif pkl_files:\n",
    "        pkl_path = os.path.join(MODEL_DIR, pkl_files[0])\n",
    "        try:\n",
    "            model = pickle.load(open(pkl_path, 'rb'))\n",
    "            print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å sklearn (pickle) –∏–∑ {pkl_files[0]}.\")\n",
    "        except Exception:\n",
    "            # –ø—Ä–æ–±—É–µ–º joblib.load\n",
    "            try:\n",
    "                model = joblib.load(pkl_path)\n",
    "                print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å sklearn (joblib) –∏–∑ {pkl_files[0]}.\")\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ sklearn-–º–æ–¥–µ–ª–∏ –∏–∑ {pkl_files[0]}:\", e2)\n",
    "    elif h5_files:\n",
    "        from tensorflow import keras\n",
    "        h5_path = os.path.join(MODEL_DIR, h5_files[0])\n",
    "        try:\n",
    "            model = keras.models.load_model(h5_path)\n",
    "            print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ Keras-–º–æ–¥–µ–ª—å –∏–∑ —Ñ–∞–π–ª–∞ {h5_files[0]}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Keras-–º–æ–¥–µ–ª–∏ {h5_files[0]}:\", e)\n",
    "    else:\n",
    "        print(\"‚ùå –í –ø–∞–ø–∫–µ –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏ .pt/.pth/.pkl/.joblib/.h5.\")\n",
    "\n",
    "# 3) –µ—Å–ª–∏ –≤—Å—ë-—Ç–∞–∫–∏ HF-–º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å ‚Äî —Ä–∞–∑–º–µ—â–∞–µ–º –Ω–∞ –Ω—É–∂–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
    "if isinstance(model, AutoModelForSequenceClassification):\n",
    "    # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if device.type == 'cuda':\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            model.to(device)\n",
    "            print(f\"‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –ø–µ—Ä–µ–º–µ—â—ë–Ω –Ω–∞ {device}.\")\n",
    "        except RuntimeError as oom:\n",
    "            if 'out of memory' in str(oom):\n",
    "                print(\"‚ö†Ô∏è CUDA OOM ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU.\")\n",
    "                device = torch.device('cpu')\n",
    "                model.to(device)\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU.\")\n",
    "    model.eval()\n",
    "else:\n",
    "    # –¥–ª—è sklearn/keras/PyTorch state-dict —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ÑπÔ∏è –ü–æ–ª—å–∑—É–µ–º—Å—è sklearn/keras/PyTorch state-dict, –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –±—É–¥–µ—Ç –Ω–∞ CPU.\")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ\n",
    "print(\"\\n== –ò—Ç–æ–≥–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ==\")\n",
    "print(\"Model object type:\", type(model))\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5078bedd-488f-41e8-a9bf-90a5887a7159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—ã–±—Ä–∞–Ω–∞ —Å–ª—É—á–∞–π–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>–°–ø–æ—Ä—Ç</th>\n",
       "      <th>–õ–∏—á–Ω–∞—è –∂–∏–∑–Ω—å</th>\n",
       "      <th>–Æ–º–æ—Ä</th>\n",
       "      <th>–°–æ—Ü—Å–µ—Ç–∏</th>\n",
       "      <th>–ü–æ–ª–∏—Ç–∏–∫–∞</th>\n",
       "      <th>–†–µ–∫–ª–∞–º–∞</th>\n",
       "      <th>–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–∏—Ä–∞—Ç—å —á—É–¥–æ, –∫–æ—Ç–æ—Ä–æ–µ —Ç–æ–ª—å–∫–æ —á—Ç–æ –ø—Ä–æ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  –°–ø–æ—Ä—Ç  –õ–∏—á–Ω–∞—è –∂–∏–∑–Ω—å  \\\n",
       "0  –¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–∏—Ä–∞—Ç—å —á—É–¥–æ, –∫–æ—Ç–æ—Ä–æ–µ —Ç–æ–ª—å–∫–æ —á—Ç–æ –ø—Ä–æ...    1.0           0.0   \n",
       "\n",
       "   –Æ–º–æ—Ä  –°–æ—Ü—Å–µ—Ç–∏  –ü–æ–ª–∏—Ç–∏–∫–∞  –†–µ–∫–ª–∞–º–∞  –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏  \n",
       "0   0.0      0.0       0.0      0.0            0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ data/processed\n",
    "test_df = None\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤\n",
    "if os.path.exists('../data/processed/test.csv'):\n",
    "    test_df = pd.read_csv('../data/processed/test.csv')\n",
    "elif os.path.exists('../data/processed/test.xlsx'):\n",
    "    test_df = pd.read_excel('../data/processed/test.xlsx')\n",
    "elif os.path.exists('../data/processed/final.xlsx'):\n",
    "    # –ï—Å–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –Ω–µ—Ç, –≤–æ–∑—å–º—ë–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ–±—â–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    full_df = pd.read_excel('../data/processed/final.xlsx')\n",
    "    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–π –∏–Ω–¥–µ–∫—Å-—Å—Ç–æ–ª–±–µ—Ü, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å\n",
    "    if 'Unnamed: 0' in full_df.columns:\n",
    "        full_df = full_df.drop('Unnamed: 0', axis=1)\n",
    "    test_df = full_df\n",
    "\n",
    "if test_df is not None and not test_df.empty:\n",
    "    # –í—ã–±–∏—Ä–∞–µ–º –æ–¥–Ω—É —Å–ª—É—á–∞–π–Ω—É—é —Å—Ç—Ä–æ–∫—É –±–µ–∑ —Ñ–∏–∫—Å–∞—Ü–∏–∏ random_state\n",
    "    test_df = test_df.sample(n=1).reset_index(drop=True)\n",
    "    print(\"–í—ã–±—Ä–∞–Ω–∞ —Å–ª—É—á–∞–π–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "    display(test_df.head())\n",
    "else:\n",
    "    print(\"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –æ–Ω–∏ –ø—É—Å—Ç—ã–µ, –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏–º–µ—Ä –≤—Ä—É—á–Ω—É—é.\")\n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä –≤—Ä—É—á–Ω—É—é\n",
    "    test_df = pd.DataFrame({\n",
    "        'full_text': [\n",
    "            \"–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –ø—Ä–æ–≤—ë–ª –∑–∞—Å–µ–¥–∞–Ω–∏–µ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏.\"\n",
    "        ]\n",
    "    })\n",
    "    display(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb72d43f-c9f7-4e16-a093-d151012da825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞–π–¥–µ–Ω—ã –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö: ['–°–ø–æ—Ä—Ç', '–õ–∏—á–Ω–∞—è –∂–∏–∑–Ω—å', '–Æ–º–æ—Ä', '–°–æ—Ü—Å–µ—Ç–∏', '–ü–æ–ª–∏—Ç–∏–∫–∞', '–†–µ–∫–ª–∞–º–∞', '–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏']\n",
      "1) –¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–∏—Ä–∞—Ç—å —á—É–¥–æ, –∫–æ—Ç–æ—Ä–æ–µ —Ç–æ–ª—å–∫–æ —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –≤ –º–µ–¥–∏–∞...\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "text_examples = []\n",
    "true_labels = None  # —Å—é–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏–º –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã (–¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)\n",
    "\n",
    "if test_df is not None:\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, 'full_text')\n",
    "    text_col = 'full_text' if 'full_text' in test_df.columns else test_df.columns[0]\n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã (–≤–æ–∑—å–º—ë–º –¥–æ 5 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏)\n",
    "    text_examples = test_df[text_col].astype(str).tolist()[:5]\n",
    "    # –ï—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–µ –±–∏–Ω–∞—Ä–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)\n",
    "    label_cols = [col for col in test_df.columns if col not in [text_col] and test_df[col].dropna().isin([0,1]).all()]\n",
    "    if label_cols:\n",
    "        true_labels = test_df[label_cols].values[:len(text_examples)]\n",
    "        print(f\"–ù–∞–π–¥–µ–Ω—ã –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö: {label_cols}\")\n",
    "else:\n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–¥–∏–Ω-–¥–≤–∞ –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞ –≤—Ä—É—á–Ω—É—é\n",
    "    text_examples = [\n",
    "        \"–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –ø—Ä–æ–≤—ë–ª –∑–∞—Å–µ–¥–∞–Ω–∏–µ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏.\",  # –æ–∂–∏–¥–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ \"–ü–æ–ª–∏—Ç–∏–∫–∞\"\n",
    "        \"–≠—Ç–æ—Ç —Ñ–∏–ª—å–º –±—ã–ª –Ω–∞—Å—Ç–æ–ª—å–∫–æ —Å–º–µ—à–Ω—ã–º, —á—Ç–æ —è —Å–º–µ—è–ª—Å—è –≤–µ—Å—å –≤–µ—á–µ—Ä.\"      # –æ–∂–∏–¥–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ \"–Æ–º–æ—Ä\"\n",
    "    ]\n",
    "    print(\"–ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤—Ä—É—á–Ω—É—é).\")\n",
    "    \n",
    "# –í—ã–≤–æ–¥–∏–º —Å–∞–º–∏ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "for i, text in enumerate(text_examples, 1):\n",
    "    print(f\"{i}) {text[:60]}{'...' if len(text) > 60 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc09b464-7bbf-4b3f-9dda-9bc69f268b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å max_length = 512\n",
      "‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞. –ö–ª—é—á–∏ –≤ inputs: dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º max_length –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "# –ï—Å–ª–∏ tokenizer.model_max_length –∑–∞–¥–∞–Ω –∏ —Ä–∞–∑—É–º–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ —Å—Ç–∞–≤–∏–º 512\n",
    "max_len = getattr(tokenizer, 'model_max_length', None)\n",
    "if not isinstance(max_len, int) or max_len > 10000:\n",
    "    max_len = 512\n",
    "print(f\"–ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å max_length = {max_len}\")\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º max_length\n",
    "inputs = tokenizer(\n",
    "    text_examples,\n",
    "    padding='max_length',   # –ø–∞–¥–¥–∏–Ω–≥ –¥–æ max_length\n",
    "    truncation=True,        # –æ–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª–∏–Ω–Ω–µ–µ max_length\n",
    "    max_length=max_len,     # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞\n",
    "    return_tensors='pt'     # PyTorch-—Ç–µ–Ω–∑–æ—Ä—ã\n",
    ")\n",
    "\n",
    "# –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(\"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞. –ö–ª—é—á–∏ –≤ inputs:\", inputs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3788bb18-fc20-4f53-a304-303456a13a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã. –§–æ—Ä–º–∞ –º–∞—Å—Å–∏–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π: (1, 7)\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "with torch.no_grad():  # –≤—ã–∫–ª—é—á–∞–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —Ç.–∫. —Å–µ–π—á–∞—Å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # –ª–æ–≥–∏—Ç—ã (–≤—ã—Ö–æ–¥—ã –¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏) —Ä–∞–∑–º–µ—Ä–æ–º [batch_size, num_labels]\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ª–æ–≥–∏—Ç—ã –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–∏–≥–º–æ–∏–¥–æ–π, —Ç.–∫. —ç—Ç–æ multi-label –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "probs_tensor = torch.sigmoid(logits)\n",
    "# –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ CPU –∏ –≤ numpy –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "probs = probs_tensor.cpu().numpy()\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –ø—Ä–∏ –ø–æ—Ä–æ–≥–µ 0.5\n",
    "pred_labels = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã. –§–æ—Ä–º–∞ –º–∞—Å—Å–∏–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π:\", probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0e44f3-1bf8-4119-be49-7d5273535d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ü—Ä–∏–º–µ—Ä 1. –¢–µ–∫—Å—Ç: –¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–∏—Ä–∞—Ç—å —á—É–¥–æ, –∫–æ—Ç–æ—Ä–æ–µ —Ç–æ–ª—å–∫–æ —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –≤ –º–µ–¥–∏–∞–ª–∏–≥–µ. —Å –∑–∞–º–µ–¥–ª–µ–Ω–Ω—ã–º –ø–æ–≤—Ç–æ—Ä–æ–º. —á–∏—Å—Ç–æ–µ –≤—Ä–µ–º—è –∏—Å—Ç–µ–∫–ª–æ, —Ç–∏—Ç–∞–Ω—É –±—ã–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã–±–∏—Ç—å –≤ –∞—É—Ç, —á—Ç–æ–±—ã –ø—Ä–æ–π—Ç–∏ –≤ —Ñ–∏–Ω–∞–ª. –∏ –æ–Ω–∏ –≤—ã–±–∏–ª–∏. –Ω–æ –Ω–∞–∑–Ω–∞—á–∏–ª–∏ —à—Ç—Ä–∞—Ñ–Ω–æ–π, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Å–ª—É—á–∏–ª—Å—è —É–≥–ª–æ–≤–æ–π - –∏ –≤—Ä–∞—Ç–∞—Ä—å —Å—ã—á–µ–≤ –∑–∞–±–∏–ª –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –≥–æ–ª. –ø—Ä–æ—Å—Ç–æ –∫—Ä–∞—Å–∞–≤–µ—Ü.  –Ω–æ. –∞ –±—ã–ª –ª–∏ —Ñ–æ–ª? –∫–∞–∫ –ø–æ –º–Ω–µ, –∑–∞—â–∏—Ç–Ω–∏–∫ –≤ –¥–≤–∏–∂–µ–Ω–∏–∏, –ø—Ä—ã–≥–∞–µ—Ç, –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç —ç–ø–∏–∑–æ–¥, –±–µ–∑ –ª–æ–∫—Ç—è –∏ —Ä—É–∫–∏ –ø–µ—Ä–≤—ã–π –Ω–∞ –º—è—á–µ. –Ω–∞–ø–∞–¥–∞—é—â–∏–π –¥–∞–∂–µ –Ω–µ –ø—Ä—ã–≥–Ω—É–ª. –Ω–æ –≤ –º–æ–º–µ–Ω—Ç –∏–≥—Ä—ã –≤ –º—è—á –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–ª–µ—á–æ–º –∑–∞–¥–µ–≤–∞–µ—Ç –≥–æ–ª–æ–≤—É —Å–æ–ø–µ—Ä–Ω–∏–∫–∞. —è –¥—É–º–∞—é, —ç—Ç–æ —á–∏—Å—Ç–∞—è –±–æ—Ä—å–±–∞.  –≤–∞—à–µ –º–Ω–µ–Ω–∏–µ? üëçüèª - —Ñ–æ–ª üëéüèª - –Ω–µ —Ñ–æ–ª  @shhhnyakin ¬´ –æ—Å (wee —É –Ω–∞—Å –±—É–¥–µ—Ç –µ—â–µ –æ–¥–Ω–∞ –∞—Ç–∞–∫–∞. –¥–∏–º–∞ —Å—ã—á–µ–≤ –≤—ã–Ω–æ—Å –º—è—á–∞, —Ü–µ–ø–ª—è—é—Ç—Å—è –¥—Ä–æ—Ç—ã –∑–∞ —ç—Ç–æ—Ç –º—è—á. –ø–æ–ª–Ω—ã–π –ª–µ–∞–ª. –≤—Å–µ! —Å–º–æ—Ç—Ä–∏–º. –≤–æ—Ç –æ–Ω —ç—Ç–æ—Ç –º–æ–º–µ–Ω—Ç. —Ä–æ–∫–æ–≤–æ–π –º–æ–º–µ–Ω—Ç, –≤–æ–∑–º–æ–∂–Ω–æ. –∏ —Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –µ—â–µ –æ—Ç –∞—Ä–±–∏–∏. —Ä–µ–∂–∏—Å—Å–µ—Ä —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏. –Ω–æ –æ–Ω –ø—Ä–∏–±–µ–∂–∞–ª. –ø—É—Å—Ç—å –∏ –Ω–µ —Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π, –Ω–æ –ø–ª—é—Å –æ–¥–∏–Ω –∏–≥—Ä–æ–∫ –≤ —à—Ç—Ä–∞—Ñ–Ω–æ–π. –∏ —ç—Ç–æ–º —Å –Ω–∞–ø–∞–¥–∞—á–µ–π. –Ω–µ—Ç, –Ω–µ —ç—Ç–æ–º, —Å —ç—Ç–æ–π –∫–∞—Ä–∞–ø—É. –Ω–µ —Å–∞–º—ã–π —Ä–æ—Å–ª—ã–π, –Ω–æ —Ç–µ—Ö–Ω–∏—á–Ω—ã–π —Ñ—É—Ç–±–æ–ª–∏—Å—Ç. —Ö–æ—Ä–æ—à–∞—è –ø–æ–¥–∞—á–∞. –º–æ–º–µ–Ω—Ç! –≥–æ–ª! –≥–æ–ª! –≥–æ–ª! –≥–æ–ª! –≥–æ–ª! –≥–æ–ª! —Ç—ã —á—Ç–æ? —Ç—ã —á—Ç–æ? –ø–æ –ø–æ–≤–æ–¥—É —ç—Ç–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞. –≥–æ—Å–ø–æ–¥–∏, –±–æ–∂–µ –º–æ–π. —ç—Ç–æ –≤–æ–æ–±—â–µ –∞–≤—Ç–æ–≥–æ–ª –∏–ª–∏ —Å—ã—á–µ–≤? –¥–∞–π—Ç–µ –Ω–∞–º –∫–∞–∫–æ–µ-—Ç–æ —Å–ª–æ—É–º–æ, –∫–æ–ª–ª–µ–≥–∏. –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ, –∫–∞–∫–æ–µ-—Ç–æ —Å–ª–æ—É–º–æ. –ø–æ—Ç–æ–º—É —á—Ç–æ —Ç–∞–º –µ—â–µ –∏ –±—É–ª—å—Ñ –±—ã–ª —Ä—è–¥–æ–º. –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ, –∫–∞–∫–æ–µ-—Ç–æ —Å–ª–æ—É–º–æ. –ø–æ—Ç–æ–º—É —á—Ç–æ –≤ —ç—Ç–æ–π —Ç–æ–ª–ø–µ, –Ω—É –∫–∞–∂–µ—Ç—Å—è, —á—Ç–æ —Å—ã—á–µ–≤. –∫–∞–∂–µ—Ç—Å—è, —á—Ç–æ —Å—ã—á–µ–≤. –Ω–æ –ø–æ-–º–æ–µ–º—É —É–¥–∞—Ä–∏–ª —Å–ª–µ–¥—É—é—â–µ–π –≥–æ–ª–æ–≤–æ–π —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–º. –Ω—É-–∫–∞! —É-—É-—É—Ö —Ç—ã, –±–æ–∂–µ –º–æ–π! —ç—Ç–æ —Å—ã—á–µ–≤! –æ–Ω –¥–æ—Ç—è–Ω—É–ª—Å—è!\n",
      "–ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –°–ø–æ—Ä—Ç\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –°–ø–æ—Ä—Ç\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–ø–æ –ø–æ—Ä—è–¥–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π): –°–ø–æ—Ä—Ç: 0.99, –õ–∏—á–Ω–∞—è –∂–∏–∑–Ω—å: 0.01, –Æ–º–æ—Ä: 0.02, –°–æ—Ü—Å–µ—Ç–∏: 0.02, –ü–æ–ª–∏—Ç–∏–∫–∞: 0.01, –†–µ–∫–ª–∞–º–∞: 0.01, –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: 0.01\n"
     ]
    }
   ],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤/—Ç–µ–º –¥–ª—è –≤—ã–≤–æ–¥–∞ (–µ—Å–ª–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω –∏–∑ –¥–∞–Ω–Ω—ã—Ö)\n",
    "if 'label_cols' not in globals():\n",
    "    label_cols = ['–°–ø–æ—Ä—Ç', '–õ–∏—á–Ω–∞—è –∂–∏–∑–Ω—å', '–Æ–º–æ—Ä', '–°–æ—Ü—Å–µ—Ç–∏', '–ü–æ–ª–∏—Ç–∏–∫–∞', '–†–µ–∫–ª–∞–º–∞', '–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏']\n",
    "\n",
    "# –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–∏–º–µ—Ä—É –∏ –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
    "for i, text in enumerate(text_examples):\n",
    "    print(f\"\\n–ü—Ä–∏–º–µ—Ä {i+1}. –¢–µ–∫—Å—Ç: {text}\")\n",
    "    # –ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã)\n",
    "    if true_labels is not None:\n",
    "        true_topics = [label_cols[j] for j, val in enumerate(true_labels[i]) if val == 1]\n",
    "        print(\"–ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:\", \", \".join(true_topics) if true_topics else \"–Ω–µ—Ç\")\n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "    pred_topics = [label_cols[j] for j, val in enumerate(pred_labels[i]) if val == 1]\n",
    "    print(\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:\", \", \".join(pred_topics) if pred_topics else \"–Ω–µ—Ç\")\n",
    "    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "    probs_str = \", \".join(f\"{label_cols[j]}: {p:.2f}\" for j, p in enumerate(probs[i]))\n",
    "    print(\"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–ø–æ –ø–æ—Ä—è–¥–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π):\", probs_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIFI Hackaton Polyglot News Analyzer (MIFI_Hackaton_Polyglot_News_Analyzer)",
   "language": "python",
   "name": "mifi_hackaton_polyglot_news_analyzer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
