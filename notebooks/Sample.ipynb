{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d36d537-9449-423c-a470-f4d1717f7e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Будет использоваться устройство: cuda\n"
     ]
    }
   ],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pickle   # для загрузки моделей sklearn в формате pickle (если понадобится)\n",
    "import joblib   # для загрузки моделей, сохранённых с joblib (если понадобится)\n",
    "\n",
    "# Определяем устройство для вычислений (CPU или GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Будет использоваться устройство:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe2ae2c-3d52-4dc9-a296-722b53ddefbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Успешно загружены токенизатор и модель Transformers из локальной папки.\n",
      "ℹ️ Пользуемся sklearn/keras/PyTorch state-dict, инференс будет на CPU.\n",
      "\n",
      "== Итоги загрузки модели ==\n",
      "Model object type: <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_DIR = '../models/final_model'\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    raise FileNotFoundError(f\"Каталог с моделью не найден: {MODEL_DIR}\")\n",
    "\n",
    "# 1) пытаемся загрузить как HF-трансформер\n",
    "model = None\n",
    "tokenizer = None\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"✅ Успешно загружены токенизатор и модель Transformers из локальной папки.\")\n",
    "except Exception as hf_err:\n",
    "    print(\"⚠️ Не удалось загрузить как Transformers-модель:\", hf_err)\n",
    "\n",
    "# 2) если не трансформер — ищем файлы с другими расширениями\n",
    "if model is None:\n",
    "    # список файлов в папке\n",
    "    files = os.listdir(MODEL_DIR)\n",
    "    # ищем PyTorch state-dict (.pt/.pth)\n",
    "    pt_files = [f for f in files if f.endswith(('.pt','.pth'))]\n",
    "    # ищем sklearn pickle/joblib (.pkl/.joblib/.sav)\n",
    "    pkl_files = [f for f in files if f.endswith(('.pkl','.joblib','.sav'))]\n",
    "    # ищем Keras H5 (.h5)\n",
    "    h5_files  = [f for f in files if f.endswith('.h5')]\n",
    "    \n",
    "    if pt_files:\n",
    "        pt_path = os.path.join(MODEL_DIR, pt_files[0])\n",
    "        try:\n",
    "            model = torch.load(pt_path, map_location='cpu')\n",
    "            print(f\"✅ Загружен PyTorch-модель из файла {pt_files[0]} на CPU.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка при загрузке PyTorch-модели {pt_files[0]}:\", e)\n",
    "    elif pkl_files:\n",
    "        pkl_path = os.path.join(MODEL_DIR, pkl_files[0])\n",
    "        try:\n",
    "            model = pickle.load(open(pkl_path, 'rb'))\n",
    "            print(f\"✅ Загружена модель sklearn (pickle) из {pkl_files[0]}.\")\n",
    "        except Exception:\n",
    "            # пробуем joblib.load\n",
    "            try:\n",
    "                model = joblib.load(pkl_path)\n",
    "                print(f\"✅ Загружена модель sklearn (joblib) из {pkl_files[0]}.\")\n",
    "            except Exception as e2:\n",
    "                print(f\"❌ Ошибка при загрузке sklearn-модели из {pkl_files[0]}:\", e2)\n",
    "    elif h5_files:\n",
    "        from tensorflow import keras\n",
    "        h5_path = os.path.join(MODEL_DIR, h5_files[0])\n",
    "        try:\n",
    "            model = keras.models.load_model(h5_path)\n",
    "            print(f\"✅ Загружена Keras-модель из файла {h5_files[0]}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка при загрузке Keras-модели {h5_files[0]}:\", e)\n",
    "    else:\n",
    "        print(\"❌ В папке нет файлов с известными расширениями .pt/.pth/.pkl/.joblib/.h5.\")\n",
    "\n",
    "# 3) если всё-таки HF-модель успешно загрузилась — размещаем на нужном устройстве\n",
    "if isinstance(model, AutoModelForSequenceClassification):\n",
    "    # определяем устройство\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if device.type == 'cuda':\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            model.to(device)\n",
    "            print(f\"✅ Трансформер перемещён на {device}.\")\n",
    "        except RuntimeError as oom:\n",
    "            if 'out of memory' in str(oom):\n",
    "                print(\"⚠️ CUDA OOM — оставляем модель на CPU.\")\n",
    "                device = torch.device('cpu')\n",
    "                model.to(device)\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        print(\"ℹ️ GPU недоступен, используем CPU.\")\n",
    "    model.eval()\n",
    "else:\n",
    "    # для sklearn/keras/PyTorch state-dict устройство управляется в процессе инференса\n",
    "    device = torch.device('cpu')\n",
    "    print(\"ℹ️ Пользуемся sklearn/keras/PyTorch state-dict, инференс будет на CPU.\")\n",
    "\n",
    "# Финальное состояние\n",
    "print(\"\\n== Итоги загрузки модели ==\")\n",
    "print(\"Model object type:\", type(model))\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5078bedd-488f-41e8-a9bf-90a5887a7159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбрана случайная строка из тестовых данных:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Спорт</th>\n",
       "      <th>Личная жизнь</th>\n",
       "      <th>Юмор</th>\n",
       "      <th>Соцсети</th>\n",
       "      <th>Политика</th>\n",
       "      <th>Реклама</th>\n",
       "      <th>Нет категории</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>давайте разбирать чудо, которое только что про...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  Спорт  Личная жизнь  \\\n",
       "0  давайте разбирать чудо, которое только что про...    1.0           0.0   \n",
       "\n",
       "   Юмор  Соцсети  Политика  Реклама  Нет категории  \n",
       "0   0.0      0.0       0.0      0.0            0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Пытаемся загрузить тестовые данные из data/processed\n",
    "test_df = None\n",
    "\n",
    "# Проверяем несколько возможных вариантов имен файлов\n",
    "if os.path.exists('../data/processed/test.csv'):\n",
    "    test_df = pd.read_csv('../data/processed/test.csv')\n",
    "elif os.path.exists('../data/processed/test.xlsx'):\n",
    "    test_df = pd.read_excel('../data/processed/test.xlsx')\n",
    "elif os.path.exists('../data/processed/final.xlsx'):\n",
    "    # Если отдельного тестового набора нет, возьмём данные из общего обработанного датасета\n",
    "    full_df = pd.read_excel('../data/processed/final.xlsx')\n",
    "    # Удаляем лишний индекс-столбец, если он есть\n",
    "    if 'Unnamed: 0' in full_df.columns:\n",
    "        full_df = full_df.drop('Unnamed: 0', axis=1)\n",
    "    test_df = full_df\n",
    "\n",
    "if test_df is not None and not test_df.empty:\n",
    "    # Выбираем одну случайную строку без фиксации random_state\n",
    "    test_df = test_df.sample(n=1).reset_index(drop=True)\n",
    "    print(\"Выбрана случайная строка из тестовых данных:\")\n",
    "    display(test_df.head())\n",
    "else:\n",
    "    print(\"Тестовые данные не найдены или они пустые, будет сгенерирован пример вручную.\")\n",
    "    # Генерируем пример вручную\n",
    "    test_df = pd.DataFrame({\n",
    "        'full_text': [\n",
    "            \"Президент провёл заседание по вопросам экономической политики.\"\n",
    "        ]\n",
    "    })\n",
    "    display(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb72d43f-c9f7-4e16-a093-d151012da825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдены метки классов в данных: ['Спорт', 'Личная жизнь', 'Юмор', 'Соцсети', 'Политика', 'Реклама', 'Нет категории']\n",
      "1) давайте разбирать чудо, которое только что произошло в медиа...\n"
     ]
    }
   ],
   "source": [
    "# Подготовка списка текстовых примеров для предсказания\n",
    "text_examples = []\n",
    "true_labels = None  # сюда сохраним истинные метки, если они известны (для демонстрации)\n",
    "\n",
    "if test_df is not None:\n",
    "    # Определяем имя колонки с текстом (предположим, 'full_text')\n",
    "    text_col = 'full_text' if 'full_text' in test_df.columns else test_df.columns[0]\n",
    "    # Получаем тексты (возьмём до 5 примеров для наглядности)\n",
    "    text_examples = test_df[text_col].astype(str).tolist()[:5]\n",
    "    # Если в данных присутствуют столбцы с метками классов (например, многоклассовые бинарные индикаторы)\n",
    "    label_cols = [col for col in test_df.columns if col not in [text_col] and test_df[col].dropna().isin([0,1]).all()]\n",
    "    if label_cols:\n",
    "        true_labels = test_df[label_cols].values[:len(text_examples)]\n",
    "        print(f\"Найдены метки классов в данных: {label_cols}\")\n",
    "else:\n",
    "    # Генерируем один-два примера текста вручную\n",
    "    text_examples = [\n",
    "        \"Президент провёл заседание по вопросам экономической политики.\",  # ожидание категории \"Политика\"\n",
    "        \"Этот фильм был настолько смешным, что я смеялся весь вечер.\"      # ожидание категории \"Юмор\"\n",
    "    ]\n",
    "    print(\"Примеры текстов для предсказания (сгенерированы вручную).\")\n",
    "    \n",
    "# Выводим сами тексты для проверки\n",
    "for i, text in enumerate(text_examples, 1):\n",
    "    print(f\"{i}) {text[:60]}{'...' if len(text) > 60 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc09b464-7bbf-4b3f-9dda-9bc69f268b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Будем использовать max_length = 512\n",
      "✅ Токенизация выполнена. Ключи в inputs: dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# Определяем max_length для токенизации\n",
    "# Если tokenizer.model_max_length задан и разумен — используем его, иначе ставим 512\n",
    "max_len = getattr(tokenizer, 'model_max_length', None)\n",
    "if not isinstance(max_len, int) or max_len > 10000:\n",
    "    max_len = 512\n",
    "print(f\"Будем использовать max_length = {max_len}\")\n",
    "\n",
    "# Токенизация с явным указанием max_length\n",
    "inputs = tokenizer(\n",
    "    text_examples,\n",
    "    padding='max_length',   # паддинг до max_length\n",
    "    truncation=True,        # обрезаем тексты длиннее max_length\n",
    "    max_length=max_len,     # максимальная длина\n",
    "    return_tensors='pt'     # PyTorch-тензоры\n",
    ")\n",
    "\n",
    "# Переносим тензоры на устройство\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(\"✅ Токенизация выполнена. Ключи в inputs:\", inputs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3788bb18-fc20-4f53-a304-303456a13a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказания получены. Форма массива вероятностей: (1, 7)\n"
     ]
    }
   ],
   "source": [
    "# Получаем предсказания модели на токенизированных данных\n",
    "with torch.no_grad():  # выключаем вычисление градиентов, т.к. сейчас инференс\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # логиты (выходы до применения функции активации) размером [batch_size, num_labels]\n",
    "\n",
    "# Преобразуем логиты в вероятности сигмоидой, т.к. это multi-label классификация\n",
    "probs_tensor = torch.sigmoid(logits)\n",
    "# Переносим на CPU и в numpy для дальнейшей обработки\n",
    "probs = probs_tensor.cpu().numpy()\n",
    "\n",
    "# Определяем предсказанные метки при пороге 0.5\n",
    "pred_labels = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"Предсказания получены. Форма массива вероятностей:\", probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0e44f3-1bf8-4119-be49-7d5273535d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Пример 1. Текст: давайте разбирать чудо, которое только что произошло в медиалиге. с замедленным повтором. чистое время истекло, титану было достаточно выбить в аут, чтобы пройти в финал. и они выбили. но назначили штрафной, после которого случился угловой - и вратарь сычев забил прекрасный гол. просто красавец.  но. а был ли фол? как по мне, защитник в движении, прыгает, выигрывает эпизод, без локтя и руки первый на мяче. нападающий даже не прыгнул. но в момент игры в мяч одновременно плечом задевает голову соперника. я думаю, это чистая борьба.  ваше мнение? 👍🏻 - фол 👎🏻 - не фол  @shhhnyakin « ос (wee у нас будет еще одна атака. дима сычев вынос мяча, цепляются дроты за этот мяч. полный леал. все! смотрим. вот он этот момент. роковой момент, возможно. и сколько будет разговоров еще от арбии. режиссер трансляции. но он прибежал. пусть и не самый высокий, но плюс один игрок в штрафной. и этом с нападачей. нет, не этом, с этой карапу. не самый рослый, но техничный футболист. хорошая подача. момент! гол! гол! гол! гол! гол! гол! ты что? ты что? по поводу этого эпизода. господи, боже мой. это вообще автогол или сычев? дайте нам какое-то слоумо, коллеги. если возможно, какое-то слоумо. потому что там еще и бульф был рядом. если возможно, какое-то слоумо. потому что в этой толпе, ну кажется, что сычев. кажется, что сычев. но по-моему ударил следующей головой соперником. ну-ка! у-у-ух ты, боже мой! это сычев! он дотянулся!\n",
      "Истинные категории: Спорт\n",
      "Предсказанные категории: Спорт\n",
      "Вероятности (по порядку категорий): Спорт: 0.99, Личная жизнь: 0.01, Юмор: 0.02, Соцсети: 0.02, Политика: 0.01, Реклама: 0.01, Нет категории: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Определяем список названий классов/тем для вывода (если не определён из данных)\n",
    "if 'label_cols' not in globals():\n",
    "    label_cols = ['Спорт', 'Личная жизнь', 'Юмор', 'Соцсети', 'Политика', 'Реклама', 'Нет категории']\n",
    "\n",
    "# Проходим по каждому примеру и выводим информацию\n",
    "for i, text in enumerate(text_examples):\n",
    "    print(f\"\\nПример {i+1}. Текст: {text}\")\n",
    "    # Истинные категории (если известны)\n",
    "    if true_labels is not None:\n",
    "        true_topics = [label_cols[j] for j, val in enumerate(true_labels[i]) if val == 1]\n",
    "        print(\"Истинные категории:\", \", \".join(true_topics) if true_topics else \"нет\")\n",
    "    # Предсказанные категории\n",
    "    pred_topics = [label_cols[j] for j, val in enumerate(pred_labels[i]) if val == 1]\n",
    "    print(\"Предсказанные категории:\", \", \".join(pred_topics) if pred_topics else \"нет\")\n",
    "    # Вероятности по каждой категории\n",
    "    probs_str = \", \".join(f\"{label_cols[j]}: {p:.2f}\" for j, p in enumerate(probs[i]))\n",
    "    print(\"Вероятности (по порядку категорий):\", probs_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIFI Hackaton Polyglot News Analyzer (MIFI_Hackaton_Polyglot_News_Analyzer)",
   "language": "python",
   "name": "mifi_hackaton_polyglot_news_analyzer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
